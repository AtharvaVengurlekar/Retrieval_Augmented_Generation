Retrieval Augmented Generation (RAG) â€“ PDF Question Answering

This repository demonstrates a Retrieval Augmented Generation (RAG) pipeline built using LangChain, enabling users to upload multiple PDF documents and ask questions that are answered strictly based on the content of those PDFs.

The system ensures no hallucinations by grounding responses only in retrieved document context.

ğŸš€ Features

Upload and process multiple PDF documents

Context-aware question answering

Local inference (no external APIs required)

Optimized for low VRAM GPUs using INT4 quantized LLMs

Strict document-grounded responses

ğŸ› ï¸ Setup Instructions
1ï¸âƒ£ Clone the Repository
git clone https://github.com/AtharvaVengurlekar/Retrieval_Augmented_Generation.git
cd Retrieval_Augmented_Generation

2ï¸âƒ£ Create & Activate Conda Environment (Python 3.10.13)
conda create -n rag_env python=3.10.13 -y
conda activate rag_env

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

ğŸ“š What is LangChain?

LangChain is a framework for building applications powered by Large Language Models (LLMs). It provides abstractions to:

Connect LLMs with external data sources (PDFs, databases, APIs)

Manage prompts, memory, and execution chains

Build Retrieval Augmented Generation (RAG) pipelines efficiently

In short, LangChain acts as the orchestration layer between your documents, embeddings, vector databases, and LLMs.

ğŸ§  Architecture Overview

ğŸ“Œ (Insert architecture diagram image here)

ğŸ”„ Application Workflow

The application follows this pipeline:

1ï¸âƒ£ Document Ingestion

Multiple PDF files are uploaded and their textual content is extracted.

2ï¸âƒ£ Text Segmentation

Extracted text is split into smaller overlapping chunks to preserve context and improve retrieval accuracy.

3ï¸âƒ£ Embedding Generation

Each text chunk is converted into a numerical vector using an embedding model.

4ï¸âƒ£ Semantic Retrieval

User queries are embedded and matched against stored vectors to retrieve the most relevant document segments.

5ï¸âƒ£ Answer Synthesis

Retrieved segments are passed to the language model, which generates answers strictly grounded in document content.

ğŸ¤– Models Used
ğŸ”¹ Large Language Model (LLM)

Mistral 7B Instruct

ğŸ”¹ Embedding Model

Sentence-Transformer: all-mpnet-base-v2

ğŸ’» Minimum Hardware Requirements
âœ… Recommended

NVIDIA RTX GPU with 6 GB+ VRAM

INT4 quantized inference for Mistral 7B or Qwen 7B

ğŸ“Š Embedding Model Comparison
Property	Instructor-XL	Sentence-Transformer (all-mpnet-base-v2)
Parameters	1.3B	110M
Embedding Dim	768	768
GPU Required	Yes	No
Speed	Slow	Fast
Memory Usage	Very High	Low
Offline Friendly	Limited	Yes
Production Stability	Medium	High
ğŸ§® LLM GPU Memory Consumption
Model	Params	FP16 / BF16	INT8	INT4
Google T5-Large	0.77B	2â€“3 GB	1.5 GB	0.8â€“1 GB
Google T5-XL	3B	6â€“8 GB	4 GB	2â€“2.5 GB
LLaMA 3.2 3B Instruct	3B	6â€“7 GB	4 GB	2â€“2.5 GB
Mistral 7B Instruct	7B	13â€“15 GB	8 GB	4â€“5 GB
Qwen 7B Instruct	7B	13â€“15 GB	8 GB	4â€“5 GB
â¬‡ï¸ Downloading Models

Download required models locally by running:

python download.py

ğŸ” Hugging Face Token (Required for Google T5 Models)

Set your Hugging Face token as an environment variable:

Windows
set HF_TOKEN=your_huggingface_token

Linux / macOS
export HF_TOKEN=your_huggingface_token

â–¶ï¸ Running the Application

Start the Streamlit app:

streamlit run main.py

ğŸ§ª Usage Steps

Upload one or more PDF documents

Click Process PDFs

Ask questions related to the uploaded content

View answers grounded strictly in the PDFs

ğŸ“Œ Expected Output

ğŸ“Œ (Insert expected output screenshot here)

ğŸ“ Notes

Designed for local inference

Optimized for INT4 quantized LLMs

Responses are strictly restricted to provided document context

âš ï¸ Limitations

Answers are limited to the content of uploaded PDFs

Large or complex documents may increase processing time

Answer quality depends on document clarity and structure
